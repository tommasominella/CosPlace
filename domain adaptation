#Innanzitutto faccio il train e salvo i risultati.
#su altre immagini (inizialmente per comodità usiamo solo tokyo night e giusto qualche immagine da sanfrancisco, da capire se aggiungere altre di dataset trained)
#alleno un modello ausiliario (discriminator) con l'obiettivo di classificare immagini che arrivano da sanfr(source) o nigth(domain, target).
#il classifier originale (del train) cercherà di massimizzare la loss del domain discriminator.
#codice seguente è un tentativo, prendendo spunto da altre repository:

import argparse
import torch
from torch import nn
import torch.nn.functional as F
from torch.utils.data import DataLoader
from torchvision.transforms import Compose, ToTensor
from tqdm import tqdm
from pathlib import Path

from datasets.train_dataset import TrainDataset #classe già creata, probabilmente sulla falsa riga delle immagini usate per train dobbiamo classificare anche le
                                                #immagini di tokyo night (senza labels in qualche modo): quindi nuova classe e nuovo parametro in input per trovare
                                                #la cartella nuova

from network import DomainNet    #(per esempio)  nuova classe da implementare, da capire come funziona: deve essere come GeoLocalizationNet oppure come GradientReversal 
                                 # di giorgio o trovato online ? qui definisco anche classifier e forward
                                 
from util import GradientReversal    #util o dove voglio salvare la classe; il "ReverseLayerF" di giorgio


DATA_DIR = Path('...')  # qua la cartella in cui ho salvato i database (tokyonigth + qualche sanfrancisco) 

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

#Preparare il dataset: da capire come prepararlo, mille opzioni
#in CosPlace è un casino: cartella augmentation usata a riga 83 di train

P_DS = torchvision.datasets.ImageFolder(DATA_DIR+"/source", transform=Compose( ToTensor()))         # SOURCE DOMAIN
T_DS = torchvision.datasets.ImageFolder(DATA_DIR+"/target", transform=Compose( ToTensor()))         # TARGET DOMAIN

#Preparare i Dataloaders (parametri a caso)
source_dataloader = DataLoader(P_DS, batch_size=BATCH_SIZE, shuffle=True, num_workers=8, drop_last=True)  #batch size la stessa usata per il train, direi anche 
target_dataloader = DataLoader(A_DS, batch_size=BATCH_SIZE, shuffle=True, num_workers=8, drop_last=True)  #num_workers

optimizer= torch.optim.Adam(model.parameters(), lr=args.lr)

for epoch in range(1, args.epochs+1):                                                  #le stesse del train
        batches = zip(source_loader, target_loader)                                    #ZERO IDEA perché
        n_batches = min(len(source_loader), len(target_loader))

        total_domain_loss = total_label_accuracy = 0
        for (source_x, source_labels), (target_x, _) in tqdm(batches, leave=False, total=n_batches):
                x = torch.cat([source_x, target_x])
                x = x.to(device)
                domain_y = torch.cat([torch.ones(source_x.shape[0]),
                                      torch.zeros(target_x.shape[0])])
                domain_y = domain_y.to(device)
                label_y = source_labels.to(device)

                features = feature_extractor(x).view(x.shape[0], -1)
                domain_preds = discriminator(features).squeeze()
                label_preds = clf(features[:source_x.shape[0]])
                
                domain_loss = F.binary_cross_entropy_with_logits(domain_preds, domain_y)
                label_loss = F.cross_entropy(label_preds, label_y)
                loss = domain_loss + label_loss

                optim.zero_grad()
                loss.backward()
                optim.step()

                total_domain_loss += domain_loss.item()
                total_label_accuracy += (label_preds.max(1)[1] == label_y).float().mean().item()

        mean_loss = total_domain_loss / n_batches
        mean_accuracy = total_label_accuracy / n_batches
        tqdm.write(f'EPOCH {epoch:03d}: domain_loss={mean_loss:.4f}, '
                   f'source_accuracy={mean_accuracy:.4f}')

        torch.save(model.state_dict(), 'trained_models/revgrad.pt')
